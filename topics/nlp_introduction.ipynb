{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Natural Language processing\n",
    "\n",
    "## Introduction to NLP\n",
    "\n",
    "### What is NLP?\n",
    "\n",
    "NLP is the study of how computers can understand, interpret and manipulate human language. \n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) and linguistics that focuses on the interaction between computers and human (natural) languages. \n",
    "\n",
    "The primary goal of NLP is to enable machines to understand, interpret, generate, and respond to human language in a way that is both meaningful and useful.\n",
    "\n",
    "![nlp](https://github.com/ValRCS/RBS_PBM771_Algorithms/raw/main/imgs/nlp_kaggle.jpg)\n",
    "src: https://www.kaggle.com/code/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud/notebook\n",
    "\n",
    "## NLP Applications\n",
    "\n",
    "NLP combines computational techniques with linguistic knowledge to develop algorithms and models capable of processing human language effectively. Some of the core tasks and challenges in NLP include:\n",
    "\n",
    "* Syntax analysis: Understanding the grammatical structure of sentences, which includes parsing and part-of-speech tagging.\n",
    "* Semantic analysis: Comprehending the meaning of words and sentences, including tasks such as word sense disambiguation, semantic role labeling, and named entity recognition.\n",
    "* Pragmatics: Interpreting the context in which language is used, such as understanding speaker intentions, resolving references, and identifying the relationships between sentences.\n",
    "* Sentiment analysis: Determining the sentiment or emotion expressed in a piece of text, such as identifying whether a statement is positive, negative, or neutral.\n",
    "* Machine translation: Automatically translating text from one language to another, taking into account linguistic nuances and cultural differences.\n",
    "* Text summarization: Generating concise summaries of longer documents, preserving the most important information.\n",
    "* Question-answering systems: Providing accurate and relevant answers to user questions based on the understanding of a corpus of text.\n",
    "* Dialogue systems: Developing conversational agents that can engage in natural and coherent interactions\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History of NLP\n",
    "\n",
    "The history of Natural Language Processing (NLP) can be traced back to the early days of computer science and artificial intelligence, evolving through several stages and breakthroughs. Here is an overview of the key milestones and developments in the field:\n",
    "\n",
    "### 1950s: Theoretical Foundations\n",
    "\n",
    "* Alan Turing proposes the Turing Test in 1950, which assesses a machine's ability to exhibit intelligent behavior indistinguishable from a human. - Turing's test is a test of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. Turing proposed that a human judge should be able to engage in a natural language conversation with a machine, and that the judge would be unable to reliably determine whether the machine was human or not. The test is now widely regarded as a failure, but it has been used as a benchmark for the development of natural language processing systems.\n",
    "* Noam Chomsky's work on transformational-generative grammar provides a formal foundation for understanding the structure of natural languages.\n",
    "\n",
    "### 1960s: Early NLP Systems - ELIZA et al\n",
    "\n",
    "* ELIZA, developed by Joseph Weizenbaum in 1964, becomes one of the first AI-based natural language dialogue systems, simulating conversation through pattern matching.\n",
    "* The first machine translation systems emerge, motivated by the Cold War and the need to translate Russian scientific texts.\n",
    "\n",
    "### 1970s: Knowledge Representation and Rule-Based Systems\n",
    "\n",
    "* SHRDLU, developed by Terry Winograd in 1972, demonstrates natural language understanding and manipulation of objects in a virtual world using an AI-based system.\n",
    "* Researchers develop rule-based systems for syntax and semantic analysis, such as Augmented Transition Networks (ATNs) and Definite Clause Grammars.\n",
    "\n",
    "### 1980s: Emergence of Statistical NLP and Machine Learning\n",
    "* The limitations of rule-based systems lead to a shift towards statistical methods and machine learning techniques for NLP.\n",
    "* IBM's Candide becomes the first data-driven, statistical machine translation system.\n",
    "\n",
    "### 1990s: The Rise of Corpus Linguistics and Computational Resources\n",
    "\n",
    "* The availability of large text corpora (e.g., the Wall Street Journal corpus and the Brown corpus) and computational resources enable the development of more advanced statistical models for NLP tasks.\n",
    "* Internet search engines become widely used, and the development of web-based NLP applications becomes possible.\n",
    "* The use of Hidden Markov Models (HMMs) and probabilistic context-free grammars gain popularity for part-of-speech tagging and syntax analysis.\n",
    "\n",
    "### 2000s: Machine Learning Advancements and the Birth of Deep Learning\n",
    "\n",
    "* Support Vector Machines (SVMs) and Maximum Entropy models become widely used for various NLP tasks, including text classification and named entity recognition.\n",
    "* The introduction of deep learning and neural networks, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, revolutionize NLP by enabling machines to learn more complex language patterns and structures.\n",
    "\n",
    "### 2010s: The Era of Pre-trained Language Models\n",
    "\n",
    "* Word embeddings, such as Word2Vec and GloVe, are introduced, allowing for the efficient representation and capturing of semantic information in words.\n",
    "* The advent of pre-trained language models, such as BERT, GPT, and ELMo, lead to significant improvements in various NLP tasks by leveraging unsupervised learning on large-scale corpora to generate contextualized word representations.\n",
    "* Transfer learning becomes a common practice, where pre-trained models are fine-tuned on specific tasks, boosting performance and reducing the need for large annotated datasets.\n",
    "\n",
    "### 2020s: Large-scale Language Models and Ethical Considerations\n",
    "\n",
    "* Continued advancements in pre-trained language models, such as GPT-3 and its successors, achieve state-of-the-art performance on a wide range of NLP benchmarks.\n",
    "* With the growing capabilities of NLP systems, ethical concerns and considerations arise, including issues related to privacy, fairness, accountability, transparency, and the potential for misuse of AI-generated content.\n",
    "* Researchers and practitioners in the field increasingly focus on mitigating biases present in language models, addressing the limitations and potential harm caused by biased data and outputs.\n",
    "* Efforts are made to develop more energy-efficient models and training techniques, as the computational cost and environmental impact of training large-scale language models become a concern.\n",
    "* Multilingual and cross-lingual models gain prominence, aiming to bridge the gap between languages and provide more inclusive access to AI technology.\n",
    "* Researchers explore methods for incorporating commonsense knowledge and external knowledge bases into NLP systems to improve their understanding and reasoning capabilities.\n",
    "\n",
    "### Future - AGI?\n",
    "\n",
    "As NLP continues to evolve, new challenges and opportunities arise, driven by the growing availability of data, advancements in computational power, and the development of novel algorithms and techniques. The field is expected to further advance in areas such as human-AI collaboration, interpretable and explainable AI, and addressing long-standing challenges in natural language understanding and generation.\n",
    "\n",
    "AGI - https://en.wikipedia.org/wiki/Artificial_general_intelligence\n",
    "\n",
    "![Future](https://waitbutwhy.com/wp-content/uploads/2015/01/Projections.png)\n",
    "\n",
    "src: https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
