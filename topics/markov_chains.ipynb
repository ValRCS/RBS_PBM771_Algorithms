{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5131pwetFPh"
      },
      "source": [
        "# Markov Chains\n",
        "\n",
        "## Introduction \n",
        "\n",
        "Markov chains are a widely used tool in natural language processing (NLP) for modeling and generating sequences of words. In NLP, a Markov chain is often used to model the probability of a sequence of words, where the probability of each word in the sequence depends only on the preceding word(s).\n",
        "\n",
        "One of the most common applications of Markov chains in NLP is in language modeling. Language modeling is the task of predicting the probability of a sequence of words. Markov chains are often used for this task because they can model the probability of a sequence of words by assuming that the probability of each word only depends on the preceding word(s). This assumption is known as the Markov assumption.\n",
        "\n",
        "For example, suppose we have a sentence \"The cat sat on the mat.\" A first-order Markov model would assume that the probability of the word \"cat\" depends only on the preceding word \"The,\" and the probability of the word \"sat\" depends only on the preceding word \"cat,\" and so on. Using this model, we can compute the probability of any given sentence by multiplying together the probabilities of each word given its preceding word(s).\n",
        "\n",
        "Markov chains can also be used for text generation, where the goal is to generate new text that follows the same statistical patterns as a given corpus of text. This is typically done by sampling from the conditional distribution of each word given its preceding word(s), using the Markov model. By iteratively sampling from this distribution, we can generate a sequence of words that has a similar statistical structure to the original text.\n",
        "\n",
        "![Markov chain](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Markovkate_01.svg/520px-Markovkate_01.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrERV18otFPm"
      },
      "source": [
        "## History of Markov Chains\n",
        "\n",
        "Markov chains are named after the Russian mathematician Andrey Markov. \n",
        "\n",
        "![Markov](https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/AAMarkov.jpg/440px-AAMarkov.jpg)\n",
        "\n",
        "Note: His son is also became a mathematician and also used initials A.A.Markov ...\n",
        "\n",
        "Markov was interested in the behavior of sequences of random variables, and he developed a mathematical framework for studying sequences where the probability of each variable depends only on the preceding variable.\n",
        "\n",
        "Before Markov there were several other mathematicians who had studied sequences of random variables, including the Bernoulli brothers, the French mathematician Pierre-Simon Laplace, and the German mathematician Karl Friedrich Gauss. \n",
        "\n",
        "\n",
        "However, Markov was the first to develop a general theory of sequences of random variables, and he was the first to use the term \"Markov chain\" to describe a sequence of random variables where the probability of each variable depends only on the preceding variable.\n",
        "\n",
        "Markov developed his theory of Markov chains in the 1900s. This theory arose from his interested in Poisson process and the theory of stochastic processes.\n",
        "\n",
        "The theory of Markov chains was further developed in the 1920s and 1930s by a number of mathematicians, including the Hungarian mathematician Alfréd Rényi and the Soviet mathematician Aleksandr Khinchin. In the mid-20th century, Markov chains became a central topic in the field of probability theory and were used to study a wide range of problems in physics, biology, economics, and other fields.\n",
        "\n",
        "In the context of natural language processing, Markov chains were first used in the 1950s and 1960s for language modeling and text generation. Early applications of Markov chains in NLP included the development of automatic language translation systems and the generation of machine-generated poetry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKcn91VWtFPm"
      },
      "source": [
        "## Core Idea behind Markov Chains\n",
        "\n",
        "The main idea behind Markov chains is to model the behavior of a sequence of random variables (such as a sequence of words in a sentence) by assuming that the probability of each variable depends only on the preceding variable (or variables).\n",
        "\n",
        "The Markov chain is defined by a set of states and a transition matrix, which specifies the probability of transitioning from one state to another. Each state represents a possible value of the random variable, and the transition matrix specifies the probabilities of moving from one state to another.\n",
        "\n",
        "The Markov property, which is central to the theory of Markov chains, states that the probability of transitioning to a new state depends only on the current state and not on any previous states. This means that the future behavior of the system is dependent only on its current state and not on any earlier history.\n",
        "\n",
        "The behavior of a Markov chain can be analyzed by studying its stationary distribution, which is the long-term probability distribution of the chain as it iterates through its state space. Under certain conditions (such as irreducibility and aperiodicity), a Markov chain will converge to a unique stationary distribution, which provides information about the long-term behavior of the chain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ITSjUZftFPn"
      },
      "source": [
        "## Python program for Markov Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OhhaDNjztFPo",
        "outputId": "37f7905f-ec86-47f7-99fb-a850a715b9f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox jumps over the lazy dog. The lazy dog also likes to sleep on the\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "class MarkovChain:\n",
        "\n",
        "    def __init__(self, order=1):\n",
        "        self.order = order\n",
        "        self.transitions = {}  # we store our transitions\n",
        "        self.starts = []\n",
        "    \n",
        "    def add_text(self, text):\n",
        "        # Split the text into words\n",
        "        words = text.split() # very simple tokenizer simply split any whitespace\n",
        "\n",
        "        # Add the start of the text to the list of possible starts\n",
        "        self.starts.append(words[:self.order])\n",
        "\n",
        "        # Add each sequence of words of length self.order to the transitions dictionary\n",
        "        for i in range(len(words) - self.order):\n",
        "            key = tuple(words[i:i+self.order])\n",
        "            value = words[i+self.order]\n",
        "            if key in self.transitions:\n",
        "                self.transitions[key].append(value)\n",
        "            else:\n",
        "                self.transitions[key] = [value]\n",
        "    \n",
        "    def generate_text(self, length):\n",
        "        # Choose a random start sequence from the list of possible starts\n",
        "        current_sequence = random.choice(self.starts)\n",
        "        \n",
        "        # Generate the next word in the sequence by choosing randomly from the possible transitions\n",
        "        # we are using a list of all possible sequence so this means the probabilities are inherent\n",
        "        generated_text = list(current_sequence)\n",
        "        for i in range(length):\n",
        "            key = tuple(current_sequence)\n",
        "            if key in self.transitions:\n",
        "                next_word = random.choice(self.transitions[key])\n",
        "            else:\n",
        "                # If we reach a sequence that is not in the transitions dictionary, choose a random start sequence\n",
        "                current_sequence = random.choice(self.starts)\n",
        "                key = tuple(current_sequence)\n",
        "                next_word = random.choice(self.transitions[key])\n",
        "            generated_text.append(next_word)\n",
        "            current_sequence = current_sequence[1:] + [next_word,]\n",
        "\n",
        "        return ' '.join(generated_text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text = \"\"\"The quick brown fox jumps over the lazy dog on the porch. \n",
        "The lazy dog then wakes up and chases the fox around the yard.\n",
        "The lazy dog also likes to sleep on the couch.\n",
        "The fox likes to sleep on the porch.\n",
        "The quick brown fox steals the dog's bone and runs away.\n",
        "\"\"\"\n",
        "\n",
        "mc = MarkovChain(order=2)\n",
        "mc.add_text(text)\n",
        "\n",
        "generated_text = mc.generate_text(length=16)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "Rr7l5LaSu9Br",
        "outputId": "62e22531-91ac-4eab-b19d-19bdbf24ed72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox jumps over the lazy dog also likes to sleep on the couch. The fox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1-hI0ywftFPr",
        "outputId": "47002f5c-2091-4494-c4c8-11319501dbf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('The', 'quick'): ['brown', 'brown'],\n",
              " ('quick', 'brown'): ['fox', 'fox'],\n",
              " ('brown', 'fox'): ['jumps', 'steals'],\n",
              " ('fox', 'jumps'): ['over'],\n",
              " ('jumps', 'over'): ['the'],\n",
              " ('over', 'the'): ['lazy'],\n",
              " ('the', 'lazy'): ['dog'],\n",
              " ('lazy', 'dog'): ['on', 'then', 'also'],\n",
              " ('dog', 'on'): ['the'],\n",
              " ('on', 'the'): ['porch.', 'couch.', 'porch.'],\n",
              " ('the', 'porch.'): ['The', 'The'],\n",
              " ('porch.', 'The'): ['lazy', 'quick'],\n",
              " ('The', 'lazy'): ['dog', 'dog'],\n",
              " ('dog', 'then'): ['wakes'],\n",
              " ('then', 'wakes'): ['up'],\n",
              " ('wakes', 'up'): ['and'],\n",
              " ('up', 'and'): ['chases'],\n",
              " ('and', 'chases'): ['the'],\n",
              " ('chases', 'the'): ['fox'],\n",
              " ('the', 'fox'): ['around'],\n",
              " ('fox', 'around'): ['the'],\n",
              " ('around', 'the'): ['yard.'],\n",
              " ('the', 'yard.'): ['The'],\n",
              " ('yard.', 'The'): ['lazy'],\n",
              " ('dog', 'also'): ['likes'],\n",
              " ('also', 'likes'): ['to'],\n",
              " ('likes', 'to'): ['sleep', 'sleep'],\n",
              " ('to', 'sleep'): ['on', 'on'],\n",
              " ('sleep', 'on'): ['the', 'the'],\n",
              " ('the', 'couch.'): ['The'],\n",
              " ('couch.', 'The'): ['fox'],\n",
              " ('The', 'fox'): ['likes'],\n",
              " ('fox', 'likes'): ['to'],\n",
              " ('fox', 'steals'): ['the'],\n",
              " ('steals', 'the'): [\"dog's\"],\n",
              " ('the', \"dog's\"): ['bone'],\n",
              " (\"dog's\", 'bone'): ['and'],\n",
              " ('bone', 'and'): ['runs'],\n",
              " ('and', 'runs'): ['away.']}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "## Let's see the transitions\n",
        "\n",
        "mc.transitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVwZj1K2tFPr"
      },
      "outputs": [],
      "source": [
        "## improvement would be to count the next words instead of using them in a list - not efficient for large corpora\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mc3 = MarkovChain(order=3)\n",
        "mc3.add_text(text)\n",
        "mc3.transitions"
      ],
      "metadata": {
        "id": "kD3nHnTFv4Mh",
        "outputId": "f10383d3-2521-402f-c22c-a3e45884d148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('The', 'quick', 'brown'): ['fox', 'fox'],\n",
              " ('quick', 'brown', 'fox'): ['jumps', 'steals'],\n",
              " ('brown', 'fox', 'jumps'): ['over'],\n",
              " ('fox', 'jumps', 'over'): ['the'],\n",
              " ('jumps', 'over', 'the'): ['lazy'],\n",
              " ('over', 'the', 'lazy'): ['dog'],\n",
              " ('the', 'lazy', 'dog'): ['on'],\n",
              " ('lazy', 'dog', 'on'): ['the'],\n",
              " ('dog', 'on', 'the'): ['porch.'],\n",
              " ('on', 'the', 'porch.'): ['The', 'The'],\n",
              " ('the', 'porch.', 'The'): ['lazy', 'quick'],\n",
              " ('porch.', 'The', 'lazy'): ['dog'],\n",
              " ('The', 'lazy', 'dog'): ['then', 'also'],\n",
              " ('lazy', 'dog', 'then'): ['wakes'],\n",
              " ('dog', 'then', 'wakes'): ['up'],\n",
              " ('then', 'wakes', 'up'): ['and'],\n",
              " ('wakes', 'up', 'and'): ['chases'],\n",
              " ('up', 'and', 'chases'): ['the'],\n",
              " ('and', 'chases', 'the'): ['fox'],\n",
              " ('chases', 'the', 'fox'): ['around'],\n",
              " ('the', 'fox', 'around'): ['the'],\n",
              " ('fox', 'around', 'the'): ['yard.'],\n",
              " ('around', 'the', 'yard.'): ['The'],\n",
              " ('the', 'yard.', 'The'): ['lazy'],\n",
              " ('yard.', 'The', 'lazy'): ['dog'],\n",
              " ('lazy', 'dog', 'also'): ['likes'],\n",
              " ('dog', 'also', 'likes'): ['to'],\n",
              " ('also', 'likes', 'to'): ['sleep'],\n",
              " ('likes', 'to', 'sleep'): ['on', 'on'],\n",
              " ('to', 'sleep', 'on'): ['the', 'the'],\n",
              " ('sleep', 'on', 'the'): ['couch.', 'porch.'],\n",
              " ('on', 'the', 'couch.'): ['The'],\n",
              " ('the', 'couch.', 'The'): ['fox'],\n",
              " ('couch.', 'The', 'fox'): ['likes'],\n",
              " ('The', 'fox', 'likes'): ['to'],\n",
              " ('fox', 'likes', 'to'): ['sleep'],\n",
              " ('porch.', 'The', 'quick'): ['brown'],\n",
              " ('brown', 'fox', 'steals'): ['the'],\n",
              " ('fox', 'steals', 'the'): [\"dog's\"],\n",
              " ('steals', 'the', \"dog's\"): ['bone'],\n",
              " ('the', \"dog's\", 'bone'): ['and'],\n",
              " (\"dog's\", 'bone', 'and'): ['runs'],\n",
              " ('bone', 'and', 'runs'): ['away.']}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mc_big = MarkovChain(order=5)\n",
        "mc_big.add_text(text)\n",
        "mc_big.transitions"
      ],
      "metadata": {
        "id": "IU6LzHNZwDTG",
        "outputId": "d0646182-3eed-4cfa-98b9-1d3acd7b5c84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('The', 'quick', 'brown', 'fox', 'jumps'): ['over'],\n",
              " ('quick', 'brown', 'fox', 'jumps', 'over'): ['the'],\n",
              " ('brown', 'fox', 'jumps', 'over', 'the'): ['lazy'],\n",
              " ('fox', 'jumps', 'over', 'the', 'lazy'): ['dog'],\n",
              " ('jumps', 'over', 'the', 'lazy', 'dog'): ['on'],\n",
              " ('over', 'the', 'lazy', 'dog', 'on'): ['the'],\n",
              " ('the', 'lazy', 'dog', 'on', 'the'): ['porch.'],\n",
              " ('lazy', 'dog', 'on', 'the', 'porch.'): ['The'],\n",
              " ('dog', 'on', 'the', 'porch.', 'The'): ['lazy'],\n",
              " ('on', 'the', 'porch.', 'The', 'lazy'): ['dog'],\n",
              " ('the', 'porch.', 'The', 'lazy', 'dog'): ['then'],\n",
              " ('porch.', 'The', 'lazy', 'dog', 'then'): ['wakes'],\n",
              " ('The', 'lazy', 'dog', 'then', 'wakes'): ['up'],\n",
              " ('lazy', 'dog', 'then', 'wakes', 'up'): ['and'],\n",
              " ('dog', 'then', 'wakes', 'up', 'and'): ['chases'],\n",
              " ('then', 'wakes', 'up', 'and', 'chases'): ['the'],\n",
              " ('wakes', 'up', 'and', 'chases', 'the'): ['fox'],\n",
              " ('up', 'and', 'chases', 'the', 'fox'): ['around'],\n",
              " ('and', 'chases', 'the', 'fox', 'around'): ['the'],\n",
              " ('chases', 'the', 'fox', 'around', 'the'): ['yard.'],\n",
              " ('the', 'fox', 'around', 'the', 'yard.'): ['The'],\n",
              " ('fox', 'around', 'the', 'yard.', 'The'): ['lazy'],\n",
              " ('around', 'the', 'yard.', 'The', 'lazy'): ['dog'],\n",
              " ('the', 'yard.', 'The', 'lazy', 'dog'): ['also'],\n",
              " ('yard.', 'The', 'lazy', 'dog', 'also'): ['likes'],\n",
              " ('The', 'lazy', 'dog', 'also', 'likes'): ['to'],\n",
              " ('lazy', 'dog', 'also', 'likes', 'to'): ['sleep'],\n",
              " ('dog', 'also', 'likes', 'to', 'sleep'): ['on'],\n",
              " ('also', 'likes', 'to', 'sleep', 'on'): ['the'],\n",
              " ('likes', 'to', 'sleep', 'on', 'the'): ['couch.', 'porch.'],\n",
              " ('to', 'sleep', 'on', 'the', 'couch.'): ['The'],\n",
              " ('sleep', 'on', 'the', 'couch.', 'The'): ['fox'],\n",
              " ('on', 'the', 'couch.', 'The', 'fox'): ['likes'],\n",
              " ('the', 'couch.', 'The', 'fox', 'likes'): ['to'],\n",
              " ('couch.', 'The', 'fox', 'likes', 'to'): ['sleep'],\n",
              " ('The', 'fox', 'likes', 'to', 'sleep'): ['on'],\n",
              " ('fox', 'likes', 'to', 'sleep', 'on'): ['the'],\n",
              " ('to', 'sleep', 'on', 'the', 'porch.'): ['The'],\n",
              " ('sleep', 'on', 'the', 'porch.', 'The'): ['quick'],\n",
              " ('on', 'the', 'porch.', 'The', 'quick'): ['brown'],\n",
              " ('the', 'porch.', 'The', 'quick', 'brown'): ['fox'],\n",
              " ('porch.', 'The', 'quick', 'brown', 'fox'): ['steals'],\n",
              " ('The', 'quick', 'brown', 'fox', 'steals'): ['the'],\n",
              " ('quick', 'brown', 'fox', 'steals', 'the'): [\"dog's\"],\n",
              " ('brown', 'fox', 'steals', 'the', \"dog's\"): ['bone'],\n",
              " ('fox', 'steals', 'the', \"dog's\", 'bone'): ['and'],\n",
              " ('steals', 'the', \"dog's\", 'bone', 'and'): ['runs'],\n",
              " ('the', \"dog's\", 'bone', 'and', 'runs'): ['away.']}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Big Order Markov Chains\n",
        "\n",
        "You can see how deterministic even 5 order Markov Chains become, there is only one choice."
      ],
      "metadata": {
        "id": "6JB0_-m3wNrm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f5ylr3EtFPs"
      },
      "source": [
        "## Limitiations of simple Markov Chains\n",
        "\n",
        "The Markov property is a strong assumption, and it is often violated in real-world applications. For example, the probability of a word in a sentence often depends on more than just the preceding word. For example, the probability of the word \"cat\" in the sentence \"The cat sat on the mat\" is likely to be higher if the preceding word is \"the\" than if the preceding word is \"sat.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59it-BDvtFPt"
      },
      "source": [
        "## Hidden Markov Chains\n",
        "\n",
        "A hidden Markov chain, also known as a hidden Markov model (HMM), is a type of Markov chain in which the state of the system is not directly observable, but instead generates a sequence of observable outputs.\n",
        "\n",
        "In a hidden Markov chain, there are two types of variables: the hidden states, which are not directly observable, and the observed outputs, which are generated by the hidden states. The hidden states represent the internal state of the system, while the observed outputs provide information about the hidden states.\n",
        "\n",
        "For example, in speech recognition, a hidden Markov chain can be used to model the speech signal as a sequence of hidden states representing different phonemes, with each hidden state generating a sequence of observable acoustic features such as spectral coefficients or mel-frequency cepstral coefficients (MFCCs).\n",
        "\n",
        "The behavior of a hidden Markov chain is defined by a transition matrix that specifies the probability of transitioning from one hidden state to another, and an emission matrix that specifies the probability of generating each observable output given the hidden state. The initial distribution over the hidden states is also specified.\n",
        "\n",
        "The key challenge in working with hidden Markov chains is to estimate the hidden states given a sequence of observed outputs. This problem, known as the decoding problem, can be solved using the Viterbi algorithm or the forward-backward algorithm.\n",
        "\n",
        "Hidden Markov chains are used in a wide range of applications, including speech recognition, natural language processing, bioinformatics, and finance. They are a powerful tool for modeling complex systems where the state of the system is not directly observable but can be inferred from a sequence of observable outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKOUwwmgtFPt"
      },
      "source": [
        "## References\n",
        "\n",
        "* https://www.americanscientist.org/article/first-links-in-the-markov-chain\n",
        "* https://www.jstor.org/stable/1403785"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXqXRbXhtFPu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}